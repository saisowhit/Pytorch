# -*- coding: utf-8 -*-
"""Copy_of_build_Generative_AI_Application_using_LlamaIndex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HbeB73FLvqmsyKNUKHvyNZ5RvcbZQbZ4
"""

!pip3 install llama-index
!pip3 install google-generativeai

from llama_index import SimpleDirectoryReader,VectorStoreIndex
from llama_index.storage.storage_context import StorageContext
from llama_index.llms.palm import Palm
from llama_index import ServiceContext
import os

documents = SimpleDirectoryReader('./data').load_data()

!mkdir -p ./data

documents

## split the text into small chunks

os.environ['GOOGLE_API_KEY']=''

llm=Palm(api_key=os.environ['GOOGLE_API_KEY'])
service_context=ServiceContext.from_defaults(llm=llm)

service_context=service_context.from_defaults(llm=llm,chunk_size=1024,chunk_overlap=20)

from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings

embed_model = HuggingFaceBgeEmbeddings(
    model_name="BAAI/bge-small-en-v1.5",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

service_context=service_context.from_defaults(embed_model=embed_model)

VectorStoreIndex.from_documents(documents,service_context=service_context)

## storing and loading index

index.storage_context.persist()
storage_context=StorageContext.from_defaults(persist_dir="./storage")
index=load_index_from_storage(storage_context)

## Q&A

response=query_engine.query("what is the meaning of life?")
response

from IPython.display import Markdown,display
display(Markdown(f"<b>{response}</b>"))

# https://www.youtube.com/watch?v=8n9M99qCDVk